package multiagent;

import Graph.Node;
import Graph.NodeStatus;
import QLearning.MAction;
import QLearning.MainClass;
import QLearning.WState;
import burlap.mdp.core.state.State;
import burlap.mdp.stochasticgames.JointAction;
import burlap.mdp.stochasticgames.model.JointRewardFunction;

public class Reward implements JointRewardFunction {

	@Override
	public double[] reward(State s, JointAction a, State sp) {

		double[] result = new double[2];

		MAction dfaction = (MAction) a.action(0);
		String dname = dfaction.getNodeName();
		String daction = dfaction.getAction();

		MAction ataction = (MAction) a.action(1);
		String aname = ataction.getNodeName();
		String aaction = ataction.getAction();

		State w = s.copy();
		WState state = (WState) w;
		int size = state.getNodeList().size();

		State wp = sp.copy();
		WState statep = (WState) wp;
		int sizep = statep.getNodeList().size();

		int dindex = 0;
		int aindex = 0;

		// Find the index of the node in the graph on which action is
		// performed.
		for (int i = 0; i < size; i++) {
			if (dname.equals(state.getNodeList().get(i).getName())) {
				dindex = i;
				break;
			}
		}
		for (int i = 0; i < sizep; i++) {
			if (aname.equals(state.getNodeList().get(i).getName())) {
				aindex = i;
				break;
			}
		}

		// The index of nodes which agents performed actions will remain same
		// in both the state that is state and statep. Therefore, there is no
		// need of creating and finding them separately.

		// Since we are implementing 2-player zero sum game. The reward of one
		// player is negative for the the another player. Over here we are
		// computing the reward for the attacker and negative of it becomes the
		// rewards for the defender. And defender wants to maximize his rewards
		// for choosing particular strategy.

		if (dindex == aindex) {
			Node n = state.getNodeList().get(dindex);
			double r = getRewardForAttacker(aaction, n, n);
			result[0] = -r;
			result[1] = r;

		} else {
			Node dn = state.getNodeList().get(dindex);
			Node an = state.getNodeList().get(aindex);
			double r = getRewardForAttacker(aaction, dn, an);
			result[0] = -r;
			result[1] = r;
		}

		return result;
	}

	// When attacker compromises a system, he receives some reward for
	// controlling that system. And same negative reward is given to defender
	// for loosing that system.
	
	public double getRewardForAttacker(String action, Node defNode,
			Node attackNode) {

		// Action is SCAN
		if (action.equals(MainClass.ACTION_SCAN)) {

			// Action is PATCH
		} else {

		}

		return 0;
	}

}
