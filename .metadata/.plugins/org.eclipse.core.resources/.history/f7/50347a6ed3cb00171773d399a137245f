package multiagent;

import Graph.Node;
import Graph.NodeStatus;
import QLearning.MAction;
import QLearning.MainClass;
import QLearning.WState;
import burlap.mdp.core.state.State;
import burlap.mdp.stochasticgames.JointAction;
import burlap.mdp.stochasticgames.model.JointRewardFunction;

public class Reward implements JointRewardFunction {

	@Override
	public double[] reward(State s, JointAction a, State sp) {

		double[] result = new double[2];

		MAction dfaction = (MAction) a.action(0);
		String dname = dfaction.getNodeName();
		String daction = dfaction.getAction();

		MAction ataction = (MAction) a.action(1);
		String aname = ataction.getNodeName();
		String aaction = ataction.getAction();

		State w = s.copy();
		WState state = (WState) w;
		int size = state.getNodeList().size();

		State wp = sp.copy();
		WState statep = (WState) wp;
		int sizep = statep.getNodeList().size();

		int dindex = 0;
		int aindex = 0;

		// Find the index of the node in the graph on which action is
		// performed.
		for (int i = 0; i < size; i++) {
			if (dname.equals(state.getNodeList().get(i).getName())) {
				dindex = i;
				break;
			}
		}
		for (int i = 0; i < sizep; i++) {
			if (aname.equals(state.getNodeList().get(i).getName())) {
				aindex = i;
				break;
			}
		}

		// The index of nodes which agents performed actions will remain same
		// in both the state that is state and statep. Therefore, there is no
		// need of creating and finding them separately.

		if (dindex == aindex) {
			// Assumption: If on the agents perform actions on the same node,
			// Defender will figure out that someone is changing the
			// functionality of the node and catch the attacker.
			result[0] = 200.0;
			result[1] = -200.0;

		} else {
			// Agents perform actions on the different node.
			Node dn = state.getNodeList().get(dindex);
			Node an = state.getNodeList().get(aindex);
			Node dnp = statep.getNodeList().get(dindex);
			Node anp = statep.getNodeList().get(aindex);
			result[0] = getRewardForDefender(daction, dn, dnp, aaction, an, anp);
			result[1] = getRewardForAttacker(daction, dn, dnp, aaction, an, anp);
		}

		return result;
	}

	public double getRewardForDefender(String defenderaction, Node dn,
			Node dnp, String attackerAction, Node an, Node anp) {

		// Action is SCAN
		if (defenderaction.equals(MainClass.ACTION_SCAN)) {

			if (dnp.getDstatus().equals(NodeStatus.HACKED))
				return MainClass.reward.get(dnp.getName()) * 2;

			if ((dn.getDstatus().equals(NodeStatus.UNKNOWN) || dnp.getDstatus()
					.equals(NodeStatus.PATCHED))
					&& dnp.getDstatus().equals(NodeStatus.VULNERABLE))
				return MainClass.reward.get(dnp.getName());

			// When dnp is either UNKNOWN or PATCHED, there is no advantage of
			// scanning that function.
			return 0;

		} else {
			// Action is PATCH

			if ((dn.getDstatus().equals(NodeStatus.VULNERABLE) || dn
					.getDstatus().equals(NodeStatus.HACKED))
					&& dnp.getDstatus().equals(NodeStatus.PATCHED))
				return MainClass.reward.get(dnp.getName()) * 3;

			// When dn status is PATCHED or UNKNOWN, agent receive 0 reward.
			if (dn.getDstatus().equals(NodeStatus.PATCHED)
					|| dn.getDstatus().equals(NodeStatus.UNKNOWN))
				return 0;

			return 0;
		}
	}

	public double getRewardForAttacker(String defenderaction, Node dn,
			Node dnp, String attackerAction, Node an, Node anp) {

		// Action is SCAN
		if (attackerAction.equals(MainClass.ACTION_SCAN)) {

			return 0;
			// Action is HACK
		} else {

			if ((an.getStatus().equals(NodeStatus.VULNERABLE))
					&& anp.getStatus().equals(NodeStatus.HACKED))
				return MainClass.reward.get(dnp.getName()) * 3;

			return 0;
		}
	}
}
