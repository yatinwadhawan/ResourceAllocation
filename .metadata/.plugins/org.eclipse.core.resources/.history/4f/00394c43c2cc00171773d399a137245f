package multiagent;

import java.lang.reflect.Array;
import java.util.ArrayList;
import java.util.List;
import java.util.Random;
import java.util.concurrent.ThreadLocalRandom;

import Graph.Node;
import Graph.NodeStatus;
import QLearning.MAction;
import QLearning.MainClass;
import QLearning.WState;
import burlap.behavior.policy.EpsilonGreedy;
import burlap.behavior.stochasticgames.GameEpisode;
import burlap.behavior.stochasticgames.PolicyFromJointPolicy;
import burlap.behavior.stochasticgames.agents.maql.MultiAgentQLearning;
import burlap.behavior.stochasticgames.agents.twoplayer.singlestage.equilibriumplayer.equilibriumsolvers.CorrelatedEquilibrium;
import burlap.behavior.stochasticgames.agents.twoplayer.singlestage.equilibriumplayer.equilibriumsolvers.MinMax;
import burlap.behavior.stochasticgames.auxiliary.GameSequenceVisualizer;
import burlap.behavior.stochasticgames.madynamicprogramming.backupOperators.CorrelatedQ;
import burlap.behavior.stochasticgames.madynamicprogramming.backupOperators.MinMaxQ;
import burlap.behavior.stochasticgames.madynamicprogramming.policies.ECorrelatedQJointPolicy;
import burlap.behavior.stochasticgames.madynamicprogramming.policies.EGreedyJointPolicy;
import burlap.behavior.stochasticgames.madynamicprogramming.policies.EGreedyMaxWellfare;
import burlap.behavior.stochasticgames.madynamicprogramming.policies.EMinMaxPolicy;
import burlap.behavior.stochasticgames.solvers.CorrelatedEquilibriumSolver;
import burlap.behavior.stochasticgames.solvers.CorrelatedEquilibriumSolver.CorrelatedEquilibriumObjective;
import burlap.domain.stochasticgames.gridgame.GGVisualizer;
import burlap.mdp.core.action.ActionType;
import burlap.mdp.core.action.UniversalActionType;
import burlap.mdp.core.state.State;
import burlap.mdp.stochasticgames.JointAction;
import burlap.mdp.stochasticgames.SGDomain;
import burlap.mdp.stochasticgames.agent.SGAgentType;
import burlap.mdp.stochasticgames.world.World;
import burlap.statehashing.HashableStateFactory;
import burlap.statehashing.simple.SimpleHashableStateFactory;
import burlap.visualizer.Visualizer;

public class WorldForMultiAgent {

	final double discount = 0.95;
	final double learningRate = 0.1;
	final double defaultQ = 100;
	double epsilon = 0.8;
	int ngames = 1;

	public static boolean isDefenderAttackerAccessingSameNode = false;
	public static SAgentType defender, attacker;
	public static Node attackerNode, defenderNode;

	public void createAndRunGameModel() {
		HashableStateFactory hashingFactory = new SimpleHashableStateFactory();
		JointWorldGenerator joint = new JointWorldGenerator();
		SGDomain domain = (SGDomain) joint.generateDomain();

		Reward rf = new Reward();
		Terminal tf = new Terminal();
		WState initialState = new WState(MainClass.nlist);

		// Creating Defender object with all the actions available to him.
		defender = new SAgentType("Defender",
				JointWorldGenerator.getDefenderActionList());

		// Adding initial actions of the attacker.
		attacker = new SAgentType("Attacker", new ArrayList<ActionType>());

		World w = new World(domain, rf, tf, initialState);

		ECorrelatedQJointPolicy policy = new ECorrelatedQJointPolicy(
				CorrelatedEquilibriumObjective.LIBERTARIAN, epsilon);

		MultiAgentQLearning dagent = new MultiAgentQLearning(domain, discount,
				learningRate, hashingFactory, defaultQ, new CorrelatedQ(
						CorrelatedEquilibriumObjective.LIBERTARIAN), true,
				defender.getTypeName(), defender);
		dagent.setLearningPolicy(new PolicyFromJointPolicy(0, policy, true));

		MultiAgentQLearning aagent = new MultiAgentQLearning(domain, discount,
				learningRate, hashingFactory, defaultQ, new CorrelatedQ(
						CorrelatedEquilibriumObjective.LIBERTARIAN), true,
				attacker.getTypeName(), attacker);
		aagent.setLearningPolicy(new PolicyFromJointPolicy(1, policy, true));

		w.join(dagent);
		w.join(aagent);

		policy.setAgentsInJointPolicyFromWorld(w);
		policy.setTargetAgent(0);

		System.out.println("Starting training");
		int randomNum;

		List<GameEpisode> games = new ArrayList<GameEpisode>();
		for (int i = 0; i < ngames; i++) {

			randomNum = ThreadLocalRandom.current().nextInt(0, 5 + 1);
			defenderNode = null;
			attackerNode = null;
			attacker.clearActions();
			MAction ms = new MAction(MainClass.nlist.get(randomNum).getName(),
					MainClass.ACTION_SCAN);
			MAction mp = new MAction(MainClass.nlist.get(randomNum).getName(),
					MainClass.ACTION_HACK);
			attacker.addAction(ms);
			attacker.addAction(mp);
			attacker.updateActionList(randomNum);

			// Initially attacker has compromised the first node which is the
			// starting point for the attacker.
			initialState.getNodeList().get(randomNum)
					.setDstatus(NodeStatus.HACKED);
			initialState.getNodeList().get(randomNum)
					.setAstatus(NodeStatus.HACKED);
			initialState.getNodeList().get(randomNum)
					.setStatus(NodeStatus.HACKED);

			GameEpisode ga = w.runGame();
			games.add(ga);
			if (i % 10 == 0) {
				System.out.println("Game: " + i + ": " + ga.maxTimeStep());
			}
		}

		System.out.println("Finished training");

		System.out.println("Analysis Starts");
		ArrayList<WState> ls = new ArrayList<WState>();
		for (int i = 0; i < games.size(); i++) {
			GameEpisode g = games.get(i);
			List<State> temp = g.getStates();
			for (int j = 0; j < temp.size(); j++) {
				if (!ls.contains(temp.get(j))) {
					ls.add((WState) temp.get(j));
				}
			}
		}
		System.out.println("Number of states covered: " + );
		System.out.println("Number of states covered: " + ls.size());

	}
}
