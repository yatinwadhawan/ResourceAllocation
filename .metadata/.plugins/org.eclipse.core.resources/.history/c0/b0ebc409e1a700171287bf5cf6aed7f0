package QLearning;

import java.util.ArrayList;
import java.util.HashMap;
import java.util.List;

import mainClass.MainClass;
import mainClass.PlotGraph;

import org.jfree.ui.RefineryUtilities;

import burlap.behavior.singleagent.Episode;
import burlap.behavior.valuefunction.ConstantValueFunction;
import burlap.behavior.valuefunction.QValue;
import burlap.mdp.core.action.Action;
import burlap.mdp.core.state.State;
import burlap.mdp.singleagent.SADomain;
import burlap.mdp.singleagent.environment.SimulatedEnvironment;
import burlap.statehashing.simple.SimpleHashableStateFactory;

public class DecisionMaking {

	public static void makeDecision() {
		// Initializing the world generator.
		WorldGenerator gen = new WorldGenerator();
		SADomain domain = (SADomain) gen.generateDomain();
		WState initialstate = new WState(nodeList);
		SimulatedEnvironment env = new SimulatedEnvironment(domain,
				initialstate);
		QLearning agent = new QLearning(domain, gamma,
				new SimpleHashableStateFactory(), new ConstantValueFunction(),
				learningrate, epsilon);

		ArrayList<State> wl = new ArrayList<State>();
		ArrayList<State> allStates = new ArrayList<State>();
		ArrayList<Double> plot = new ArrayList<Double>();
		HashMap<State, List<List<QValue>>> map = new HashMap<State, List<List<QValue>>>();

		// run Q-learning and store results in a list
		List<Episode> episodes = new ArrayList<Episode>(trials);
		for (int x = 0; x < trials; x++) {
			episodes.add(agent.runLearningEpisode(env));
			env.resetEnvironment();
			Episode e = episodes.get(x);
			List<State> sl = e.stateSequence;

			for (int j = 0; j < sl.size(); j++) {
				WState w = (WState) sl.get(j);
				if (!wl.contains(w)) {
					List<List<QValue>> qv = new ArrayList<List<QValue>>();
					wl.add(sl.get(j));
					qv.add(agent.qValues(sl.get(j)));
					map.put(sl.get(j), qv);
				} else {
					List<List<QValue>> qv = map.get(sl.get(j));
					qv.add(agent.qValues(sl.get(j)));
					map.put(sl.get(j), qv);
				}
			}
		}
		System.out.println("Count States - " + count_state);
		System.out.println("Total states - " + allStates.size());
		wl.clear();
		allStates.clear();
		for (int i = 0; i < trials; i++) {
			Episode e = episodes.get(i);
			List<Action> al = e.actionSequence;
			List<Double> rl = e.rewardSequence;
			List<State> sl = e.stateSequence;

			for (int j = 0; j < sl.size(); j++) {
				WState w = (WState) sl.get(j);
				if (!wl.contains(w)) {
					wl.add(sl.get(j));
					allStates.add(sl.get(j));
				}
			}
			double reward = 0.0;
			for (int j = 0; j < rl.size(); j++) {
				reward += rl.get(j);
			}
			plot.add(reward / rl.size());

			writeFile(al, MainClass.ADDRESS + "action.text", i);
			writeFile(rl, MainClass.ADDRESS + "reward.text", i);
		}

		System.out.println("Count States - " + MainClass.count_state);
		System.out.println("Total states - " + allStates.size());

		PlotGraph demo = new PlotGraph("Average Reward per Episode", plot);
		demo.pack();
		RefineryUtilities.centerFrameOnScreen(demo);
		demo.setVisible(true);

		purgeDirectory(MainClass.ADDRESS + "/states/");
		writeAllStates(allStates, MainClass.ADDRESS + "states.text");
		createFilesForEachStateQValues(map);
	}

}
